import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from typing import Dict

MODEL_NAME = "tiiuae/falcon-7b-instruct"
DEFAULT_MAX_LENGTH = 128


class Model:
    """" https://towardsdatascience.com/deploying-falcon-7b-into-production-6dd28bb79373 """
    def __init__(self, **kwargs) -> None:
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print("THE DEVICE INFERENCE IS RUNNING ON IS: ", self.device)
        self.tokenizer = None
        self.pipeline = None

    def load(self):
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        model_8bit = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="auto",
            load_in_8bit=True,
            trust_remote_code=True)

        self.pipeline = pipeline(
            "text-generation",
            model=model_8bit,
            tokenizer=self.tokenizer,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            device_map="auto",
        )

    def predict(self, request: Dict) -> Dict:
        with torch.no_grad():
            prompt = request.pop("prompt")
            data = self.pipeline(
                prompt,
                eos_token_id=self.tokenizer.eos_token_id,
                max_length=DEFAULT_MAX_LENGTH,
                **request
            )[0]
            return {"data": data}


def text_generation(prompt):
    """Given a type of meditation, outputs a script generated by the model"""

    model = Model()
    model.load()
    output = model.predict({"prompt": prompt})
    response = output['data']['generated_text']

    # Deleting the prompt
    lines = response.split('\n')
    response = '\n'.join(lines[1:])
    print(response)

    return response
