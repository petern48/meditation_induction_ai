import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from typing import Dict
import re

MODEL_NAME = 'NousResearch/Llama-2-7b-chat-hf'
DEFAULT_MAX_LENGTH = 128


class Model:
    """" https://huggingface.co/NousResearch/Llama-2-7b-chat-hf """
    def __init__(self, **kwargs) -> None:
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print("THE DEVICE INFERENCE IS RUNNING ON IS: ", self.device)
        self.tokenizer = None
        self.pipeline = None

    def load(self):
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "right"

        model_8bit = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            device_map="auto",
            load_in_8bit=True,
            trust_remote_code=True)

        self.pipeline = pipeline(
            "text-generation",
            model=model_8bit,
            tokenizer=self.tokenizer,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            device_map="auto",
        )

    def predict(self, request: Dict) -> Dict:
        with torch.no_grad():
            prompt = request.pop("prompt")
            data = self.pipeline(
                prompt,
                eos_token_id=self.tokenizer.eos_token_id,
                max_length=DEFAULT_MAX_LENGTH,
                **request
            )[0]
            return {"data": data}


def text_generation(tokenized_prompt):
    """Given a type of meditation, outputs a script generated by the model"""

    model = Model()
    model.load()
    output = model.predict({"prompt": tokenized_prompt})
    response = output['data']['generated_text']

    # Deleting the prompt  (OLD for falcon 7b)
    # lines = response.split('\n')
    # response = '\n'.join(lines[1:])

    # Extract the actual script (remove the prompt)
    script = re.sub(r'[\s\S]*\[/INST\].*', '', output).lstrip()
    # ^ Any char followed by [/INST] followed by any non-newline char (.*)
    # The rest of the line usually says 'Great! Here's a meditation script that ...
    print(f'### PROMPT:\n{tokenized_prompt}\n\n')


    # # If response did not end in full sentence, remove the last substring that does not end in a period
    # last_period_idx = script.rfind('.')
    # if last_period_idx != -1:
    #     script = script[:last_period_idx + 1]  # + 1 to keep the period

    print(script)

    return script


def get_prompt(med_type, context):
    """Returns a tokenized prompt for the given meditation type and context"""

    context += "\nMake sure the response ends with a complete sentence."

    GOALS = {
        'mindful observation': 'emphasizes the practice of mindfully observing oneself in a state of stillness, which includes lying meditation, sitting in silence, and observing thoughts and emotions.',
        'body-centered meditation': 'focuses on heightened awareness of the body, breath, and sensory perceptions, sometimes involving concentration on specific body locations or energy centers.',
        'visual concentration': 'involves techniques like visualizations and concentration on visual objects to achieve a centered state of mind.',
        'contemplation': 'encourages deep reflection on contradictions, paradoxes, or spiritual questions.',
        'affect-centered meditation': 'encompasses practices that cultivate positive emotions such as compassion, loving-kindness, and equanimity.',
        'mantra meditation': 'involves the repetitive use of syllables, words, or phrases, often with sounds and mantras, to facilitate meditation.',
        'movement meditation': 'combines mindfulness with physical activity, including manipulating the breath, walking, and sensory observation.'
    }

    prompt = f"Write me a {med_type} meditation script of about 200 words which {GOALS[med_type]}"

    tokenized_prompt = f"<s>[INST] <<SYS>>\n{context}\n<</SYS>>\n\n{prompt} [/INST]"

    return tokenized_prompt