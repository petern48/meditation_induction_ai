from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import GPT2LMHeadModel,  GPT2Tokenizer
import transformers
import torch

# falcon_model = AutoModelForCausalLM.from_pretrained("tiiuae/falcon-7b-instruct", trust_remove_code=True)

def text_generation(selected_type):
    """Given a type of meditation, outputs a script generated by the model"""
    tokenizer = AutoTokenizer.from_pretrained("tiiuae/falcon-7b-instruct")
    pipeline = transformers.pipeline(
        "text-generation",
        model="tiiuae/falcon-7b-instruct",
        tokenizer=tokenizer,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        device_map="auto",
    )

    prompts = {
        'focused': 'write me a focused meditation script designed to enhance focus and attention by noticing all 5 senses',
        'body-scan': 'write me a body scan meditation script designed to relax and relieve stress by tightening and relaxing muscles',
        'visualization': 'write me a visualization meditation script designed to boost mood, reduce stress, and promote inner peace by noticing all 5 senses at the beach',  # or garden
        'reflection': 'write me a reflection meditation script designed to increase self awareness, mindfulness, and gratitude by asking the user about the current day and the recent past',
        'movement': 'write me a movement meditation script designed to improve mind body connection, energy, vitality, and the systems of the body'
    }

    selected_prompt = prompts[selected_type]

    sequences = pipeline(
        selected_prompt,
        # max_length=200,
        max_new_tokens=600,
        do_sample=True,
        top_k=10,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id
    )
    # Should only be one seq
    for seq in sequences:
        output = seq['generated_text']

        # Remove the 1st line (prompt), from the string
        response = '\n'.join(output.split('\n')[1:])

    return response

# def falcon_model(selected_type):



def saved_model(selected_type="body-scan"):
    model_dir = './saved_model/'
    model = GPT2LMHeadModel.from_pretrained(model_dir)
    tokenizer = GPT2Tokenizer.from_pretrained(model_dir)

    device = torch.device("cpu")  # USING CPU FOR NOW

    model.to(device)
    model.eval()
    # prompt = f"<|startoftext|> [{selected_type.upper()} MEDITATION] Welcome This {selected_type} meditation script is designed for..."

    prompt = "<|startoftext|> [BODY-SCAN MEDITATION] Welcome. This body scan meditation script is designed for preparing you for sleep. Start by flexing and relaxing your"
    # prompt = "<|startoftext|> [BODY-SCAN MEDITATION]"


    # end_prompt = "Thank you for joining me for this meditation."

    input_tensor = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)

    # end_input_tensor = torch.tensor(tokenizer.encode(end_prompt)).unsqueeze(0).to(device)

    sample_outputs = model.generate(
                                    input_tensor,
                                    #bos_token_id=random.randint(1,30000),
                                    do_sample=True,
                                    top_k=1000,
                                    max_length = 512,
                                    top_p=0.95,
                                    num_return_sequences=1
                                    )

    # end_outputs = model.generate(
    #                                 end_input_tensor,
    #                                 #bos_token_id=random.randint(1,30000),
    #                                 do_sample=True,
    #                                 top_k=1000,
    #                                 max_length = 300,
    #                                 top_p=0.95,
    #                                 num_return_sequences=1
    #                                 )

    response = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)
    print(response)



if __name__=='__main__':
    saved_model()
